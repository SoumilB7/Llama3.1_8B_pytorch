{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LLAMA 3.1 8B</h3>\n",
    "entirely in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# Entire Architecture \n",
    "\n",
    "n_layer = 32\n",
    "\n",
    "\n",
    "\n",
    "# Input customizations\n",
    "\n",
    "vocab_sz  = 128256  # 100k for english bpe tokens and 28k for multilingual tokens\n",
    "n_embd = 4096\n",
    "n_pos_emb = 8096  # same as block size \n",
    "RoPE_theta = 50000.0\n",
    "\n",
    "\n",
    "# Transformer Block - FFN\n",
    "\n",
    "batch_size = 128  # sequences going in parallel\n",
    "block_size = 8192 # sequece length (context length)\n",
    "ffn_multplier = 256 # we need layers having matrices with dimensions that are multiple of 256\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Transformer Block - Attention - Grouped multi query\n",
    "\n",
    "n_head = 32\n",
    "n_head_kv = 64\n",
    "num_key_value_heads = 8\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Each layer of transformer\n",
    "\n",
    "\n",
    "class GMQAttention(nn.module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = n_head_kv\n",
    "        self.n_q_heads = n_head\n",
    "        # to complete\n",
    "\n",
    "\n",
    "class FeedForward(nn.module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layer dimensions\n",
    "        ffn_temp = 4*(int(2*n_embd/3))\n",
    "        hidden_embd = ffn_multplier *((ffn_temp + n_embd-1) //ffn_multplier)  # rounded to nearest 256 multiple\n",
    "\n",
    "        \n",
    "        self.l1_a = nn.Linear(n_embd,hidden_embd,bias=False)\n",
    "        self.l2 = nn.Linear(hidden_embd,n_embd,bias=False)\n",
    "        self.l1_l = nn.Linear(n_embd,hidden_embd,bias=False)\n",
    "        \n",
    "    def forward(self,x:torch.Tensor):\n",
    "        \n",
    "        # creating 2 seperate forms of x\n",
    "        x_swish = nn.SiLU(self.l1_a(x)) # 1 with activation\n",
    "        x_lin = self.l1_l(x) # 2 without activation\n",
    "        x = x_swish * x_lin # multiplying it for fianl input \n",
    "        \n",
    "        x = self.l2(x) # final linear layer with the input\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Attention \n",
    "        head_sz = n_embd // n_head\n",
    "        self.norm1 = nn.RMSNorm(n_embd)\n",
    "        self.atten = GMQAttention(n_head,head_sz)\n",
    "        \n",
    "        # MLP\n",
    "        self.norm2 = nn.RMSNorm(n_embd)\n",
    "        self.line = FeedForward(n_embd)\n",
    "    \n",
    "\n",
    "    def forward(self,idx):\n",
    "        # applying attention\n",
    "        atn = self.atten(self.norm1(idx))\n",
    "\n",
    "        # applying feed forward\n",
    "        lin = self.line(self.norm2(idx))\n",
    "        \n",
    "        # Adding them instead of replacing in order to keep the original input as context\n",
    "        idx = idx + atn\n",
    "        idx = idx + lin\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n",
    "# Entire Model husk\n",
    "\n",
    "class Llama(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding table for all tokens in vocab along n_embd dimensions\n",
    "        self.token_embedding = nn.Embedding(vocab_sz, n_embd)  \n",
    "        \n",
    "        #  RoPE embedding table for all positions in context length along n_embd dimensions \n",
    "        # self.position_embedding = RoPE(n_pos_emb, )    # Will do later\n",
    "        self.pos_embedding = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # Blocks into a sequential model\n",
    "        self.Layers = nn.Sequential(*[Layer(n_embd, n_head=n_head) for _ in range(n_layer)])  # 32 layers\n",
    "\n",
    "        # Nomalization\n",
    "        self.finalnorm = nn.RMSNorm(n_embd)\n",
    "\n",
    "        # Linear\n",
    "        self.finallin = nn.Linear(n_embd,vocab_sz)\n",
    "\n",
    "        # for applying residues\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "\n",
    "    # The application layer\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape  # idx is a random set of examples chosen from dataset\n",
    "        # B number of examples of T length each\n",
    "\n",
    "        tok_embds = self.token_embedding(idx) # [B,T] -> [B,T,C]\n",
    "        pos_embds = self.pos_embedding(torch.arange(T, device=device))   # [T] -> [T,C]\n",
    "        # Since position embedding does not depend upon the input, it is added equally to all B's\n",
    "\n",
    "        x = tok_embds + pos_embds\n",
    "        # Embedding done\n",
    "\n",
    "        x = self.Layers(x)  # [B,T,C] -> [B,T,C]\n",
    "        # Repeating transformer done\n",
    "\n",
    "        x = self.finalnorm(x)  # [B,T,C] -> [B,T,C]\n",
    "        # Final RMS norm done\n",
    "\n",
    "        x = self.finallin(x)   # [B,T,C] -> [B,T,vocab_sz] \n",
    "        # final linear, this converts embeddings to vocab\n",
    "\n",
    "        return x\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
