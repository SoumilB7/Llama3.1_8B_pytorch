{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LLAMA 3.1 8B</h3>\n",
    "entirely in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Program Files\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Program Files\\Python312\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Program Files\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\soumi\\AppData\\Local\\Temp\\ipykernel_143292\\550838071.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\__init__.py\", line 2120, in <module>\n",
      "    from torch._higher_order_ops import cond\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_higher_order_ops\\__init__.py\", line 1, in <module>\n",
      "    from .cond import cond\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_higher_order_ops\\cond.py\", line 5, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 42, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 258, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "C:\\Users\\soumi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# Entire Architecture \n",
    "\n",
    "n_layer = 32\n",
    "\n",
    "\n",
    "\n",
    "# Input customizations\n",
    "\n",
    "vocab_sz  = 128256  # 100k for english bpe tokens and 28k for multilingual tokens\n",
    "n_embd = 4096\n",
    "n_pos_emb = 8096  # same as block size \n",
    "RoPE_theta = 50000.0\n",
    "\n",
    "\n",
    "# Transformer Block - FFN\n",
    "\n",
    "batch_size = 128  # sequences going in parallel\n",
    "block_size = 8192 # sequece length (context length)\n",
    "ffn_multplier = 256 # we need layers having matrices with dimensions that are multiple of 256\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Transformer Block - Attention - Grouped multi query\n",
    "\n",
    "n_head = 32\n",
    "n_head_kv = 64\n",
    "num_key_value_heads = 8\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Each layer of transformer\n",
    "\n",
    "\n",
    "class GMQAttention(nn.module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = n_head_kv\n",
    "        self.n_q_heads = n_head\n",
    "        # to complete\n",
    "\n",
    "\n",
    "class FeedForward(nn.module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # layer dimensions\n",
    "        ffn_temp = 4*(int(2*n_embd/3))\n",
    "        hidden_embd = ffn_multplier *((ffn_temp + n_embd-1) //ffn_multplier)  # rounded to nearest 256 multiple\n",
    "\n",
    "        \n",
    "        self.l1_a = nn.Linear(n_embd,hidden_embd,bias=False)\n",
    "        self.l2 = nn.Linear(hidden_embd,n_embd,bias=False)\n",
    "        self.l1_l = nn.Linear(n_embd,hidden_embd,bias=False)\n",
    "        \n",
    "    def forward(self,x:torch.Tensor):\n",
    "        \n",
    "        # creating 2 seperate forms of x\n",
    "        x_swish = nn.SiLU(self.l1_a(x)) # 1 with activation\n",
    "        x_lin = self.l1_l(x) # 2 without activation\n",
    "        x = x_swish * x_lin # multiplying it for fianl input \n",
    "        \n",
    "        x = self.l2(x) # final linear layer with the input\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Attention \n",
    "        head_sz = n_embd // n_head\n",
    "        self.norm1 = nn.RMSNorm(n_embd)\n",
    "        self.atten = GMQAttention(n_head,head_sz)\n",
    "        \n",
    "        # MLP\n",
    "        self.norm2 = nn.RMSNorm(n_embd)\n",
    "        self.line = FeedForward(n_embd)\n",
    "    \n",
    "\n",
    "    def forward(self,idx):\n",
    "        # applying attention\n",
    "        atn = self.atten(self.norm1(idx))\n",
    "\n",
    "        # applying feed forward\n",
    "        lin = self.line(self.norm2(idx))\n",
    "        \n",
    "        # Adding them instead of replacing in order to keep the original input as context\n",
    "        idx = idx + atn\n",
    "        idx = idx + lin\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n",
    "# Entire Model husk\n",
    "\n",
    "class Llama(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding table for all tokens in vocab along n_embd dimensions\n",
    "        self.token_embedding = nn.Embedding(vocab_sz, n_embd)  \n",
    "        \n",
    "        #  RoPE embedding table for all positions in context length along n_embd dimensions \n",
    "        # self.position_embedding = RoPE(n_pos_emb, )    # Will do later\n",
    "        self.pos_embedding = nn.Embedding(block_size, n_embd)\n",
    "\n",
    "        # Blocks into a sequential model\n",
    "        self.Layers = nn.Sequential(*[Layer(n_embd, n_head=n_head) for _ in range(n_layer)])  # 32 layers\n",
    "\n",
    "        # Nomalization\n",
    "        self.finalnorm = nn.RMSNorm(n_embd)\n",
    "\n",
    "        # Linear\n",
    "        self.finallin = nn.Linear(n_embd,vocab_sz)\n",
    "\n",
    "        # for applying residues\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "\n",
    "    # The application layer\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape  # idx is a random set of examples chosen from dataset\n",
    "        # B number of examples of T length each\n",
    "\n",
    "        tok_embds = self.token_embedding(idx) # [B,T] -> [B,T,C]\n",
    "        pos_embds = self.pos_embedding(torch.arange(T, device=device))   # [T] -> [T,C]\n",
    "        # Since position embedding does not depend upon the input, it is added equally to all B's\n",
    "\n",
    "        x = tok_embds + pos_embds\n",
    "        # Embedding done\n",
    "\n",
    "        x = self.Layers(x)  # [B,T,C] -> [B,T,C]\n",
    "        # Repeating transformer done\n",
    "\n",
    "        x = self.finalnorm(x)  # [B,T,C] -> [B,T,C]\n",
    "        # Final RMS norm done\n",
    "\n",
    "        x = self.finallin(x)   # [B,T,C] -> [B,T,vocab_sz] \n",
    "        # final linear, this converts embeddings to vocab\n",
    "\n",
    "        return x\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
